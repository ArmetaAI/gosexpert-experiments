{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaBSE Classification for Document Tagging\n",
    "\n",
    "This notebook uses LaBSE (Language-agnostic BERT Sentence Embedding) for document classification.\n",
    "\n",
    "## Approach:\n",
    "- Use LaBSE to generate multilingual embeddings\n",
    "- Train classifiers on top of frozen LaBSE embeddings\n",
    "- Compare with lightweight classifier fine-tuning\n",
    "- Local execution\n",
    "\n",
    "## Setup:\n",
    "- Model: `sentence-transformers/LaBSE`\n",
    "- GCS Bucket: `gosexpert_categorize`\n",
    "- Split: 70% train / 30% test\n",
    "- MLflow: Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project root\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ML Models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# GCS and MLflow\n",
    "from gcs_bucket_interface import GCSBucketInterface\n",
    "from mlflow_recorder import MLflowRecorder\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "EXPERIMENT_NAME = \"labse_classification_v1\"\n",
    "BUCKET_NAME = \"gosexpert_categorize\"\n",
    "TRAIN_TEST_SPLIT = 0.3\n",
    "RANDOM_STATE = 42\n",
    "PAGES_TO_EXTRACT = 3\n",
    "\n",
    "# Model configuration\n",
    "EMBEDDING_MODEL = \"sentence-transformers/LaBSE\"\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Train/Test Split: {int((1-TRAIN_TEST_SPLIT)*100)}/{int(TRAIN_TEST_SPLIT*100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GCS\n",
    "gcs_interface = GCSBucketInterface(bucket_name=BUCKET_NAME)\n",
    "\n",
    "print(\"Loading files from GCS...\")\n",
    "all_files = gcs_interface.list()\n",
    "\n",
    "# Filter PDFs with tags\n",
    "pdf_files_with_tags = []\n",
    "for file_info in all_files:\n",
    "    if file_info['name'].lower().endswith('.pdf') and file_info.get('metadata'):\n",
    "        metadata = file_info['metadata']\n",
    "        if any(key.startswith('tag') or 'category' in key.lower() for key in metadata.keys()):\n",
    "            pdf_files_with_tags.append(file_info)\n",
    "\n",
    "print(f\"Found {len(pdf_files_with_tags)} PDF files\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_files = pd.DataFrame([{\n",
    "    'file_name': f['name'],\n",
    "    'gcs_uri': f['gcs_uri'],\n",
    "    'tags': [v for k, v in f['metadata'].items() if 'tag' in k.lower()]\n",
    "} for f in pdf_files_with_tags])\n",
    "\n",
    "print(f\"DataFrame: {df_files.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EXTRACTION_DIR = project_root / \"text_extraction_results\"\n",
    "\n",
    "def load_text_from_json(json_filename: str, max_pages: int = PAGES_TO_EXTRACT) -> str:\n",
    "    json_path = TEXT_EXTRACTION_DIR / json_filename\n",
    "    if not json_path.exists():\n",
    "        return \"\"\n",
    "    try:\n",
    "        import json\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            page_data = json.load(f)\n",
    "        text_parts = [page_data[f\"page{i}\"] for i in range(1, max_pages+1) if f\"page{i}\" in page_data]\n",
    "        return \"\\n\".join(text_parts).strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Loading text...\")\n",
    "texts, valid_indices = [], []\n",
    "for idx, row in df_files.iterrows():\n",
    "    text = load_text_from_json(row['file_name'].replace('.pdf', '.json'))\n",
    "    if text:\n",
    "        texts.append(text)\n",
    "        valid_indices.append(idx)\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df_files)}...\")\n",
    "\n",
    "df_files = df_files.loc[valid_indices].reset_index(drop=True)\n",
    "df_files['text'] = texts\n",
    "print(f\"Loaded {len(df_files)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Labels and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags\n",
    "def get_primary_tag(tags_list):\n",
    "    return tags_list[0] if isinstance(tags_list, list) and len(tags_list) > 0 else \"unknown\"\n",
    "\n",
    "df_files['primary_tag'] = df_files['tags'].apply(get_primary_tag)\n",
    "df_files = df_files[df_files['primary_tag'] != \"unknown\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Documents: {len(df_files)}\")\n",
    "print(f\"Unique tags: {df_files['primary_tag'].nunique()}\")\n",
    "print(f\"\\nTop tags:\")\n",
    "print(df_files['primary_tag'].value_counts().head(10))\n",
    "\n",
    "# Train/test split\n",
    "tag_counts = df_files['primary_tag'].value_counts()\n",
    "rare_tags = tag_counts[tag_counts < 2].index\n",
    "valid_tags = tag_counts[tag_counts >= 2].index\n",
    "\n",
    "if len(rare_tags) > 0:\n",
    "    print(f\"\\n⚠️ {len(rare_tags)} rare tags\")\n",
    "    rare_df = df_files[df_files['primary_tag'].isin(rare_tags)]\n",
    "    valid_df = df_files[df_files['primary_tag'].isin(valid_tags)]\n",
    "    train_df, test_df = train_test_split(\n",
    "        valid_df, test_size=TRAIN_TEST_SPLIT, random_state=RANDOM_STATE,\n",
    "        stratify=valid_df['primary_tag']\n",
    "    )\n",
    "    train_df = pd.concat([train_df, rare_df], ignore_index=True)\n",
    "else:\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_files, test_size=TRAIN_TEST_SPLIT, random_state=RANDOM_STATE,\n",
    "        stratify=df_files['primary_tag']\n",
    "    )\n",
    "\n",
    "print(f\"\\nSplit: Train={len(train_df)} | Test={len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate LaBSE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading LaBSE model: {EMBEDDING_MODEL}...\")\n",
    "print(\"Note: This may take a few minutes on first run...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Generate train embeddings\n",
    "print(\"\\nGenerating train embeddings...\")\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_embeddings = embedding_model.encode(\n",
    "    train_texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=16,  # Reduce if OOM\n",
    "    normalize_embeddings=True  # L2 normalization for better similarity\n",
    ")\n",
    "print(f\"Train embeddings: {train_embeddings.shape}\")\n",
    "\n",
    "# Generate test embeddings\n",
    "print(\"\\nGenerating test embeddings...\")\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_embeddings = embedding_model.encode(\n",
    "    test_texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=16,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "print(f\"Test embeddings: {test_embeddings.shape}\")\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df['primary_tag'].values\n",
    "y_test = test_df['primary_tag'].values\n",
    "\n",
    "print(f\"\\nClasses: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Multiple Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "xgb_model.fit(train_embeddings, y_train_enc)\n",
    "xgb_pred_enc = xgb_model.predict(test_embeddings)\n",
    "xgb_pred = label_encoder.inverse_transform(xgb_pred_enc)\n",
    "\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "xgb_f1 = f1_score(y_test, xgb_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {xgb_acc:.4f}\")\n",
    "print(f\"F1 Score: {xgb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=25,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "rf_model.fit(train_embeddings, y_train)\n",
    "rf_pred = rf_model.predict(test_embeddings)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "rf_f1 = f1_score(y_test, rf_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {rf_acc:.4f}\")\n",
    "print(f\"F1 Score: {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "lr_model.fit(train_embeddings, y_train)\n",
    "lr_pred = lr_model.predict(test_embeddings)\n",
    "\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "lr_f1 = f1_score(y_test, lr_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {lr_acc:.4f}\")\n",
    "print(f\"F1 Score: {lr_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LINEAR SVM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "svm_model = LinearSVC(\n",
    "    C=1.0,\n",
    "    max_iter=2000,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "svm_model.fit(train_embeddings, y_train)\n",
    "svm_pred = svm_model.predict(test_embeddings)\n",
    "\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "svm_f1 = f1_score(y_test, svm_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {svm_acc:.4f}\")\n",
    "print(f\"F1 Score: {svm_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 MLP Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MLP NEURAL NETWORK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256, 128),\n",
    "    activation='relu',\n",
    "    max_iter=500,\n",
    "    learning_rate='adaptive',\n",
    "    early_stopping=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "mlp_model.fit(train_embeddings, y_train)\n",
    "mlp_pred = mlp_model.predict(test_embeddings)\n",
    "\n",
    "mlp_acc = accuracy_score(y_test, mlp_pred)\n",
    "mlp_f1 = f1_score(y_test, mlp_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {mlp_acc:.4f}\")\n",
    "print(f\"F1 Score: {mlp_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'Random Forest', 'Logistic Regression', 'Linear SVM', 'MLP'],\n",
    "    'Accuracy': [xgb_acc, rf_acc, lr_acc, svm_acc, mlp_acc],\n",
    "    'F1 Score': [xgb_f1, rf_f1, lr_f1, svm_f1, mlp_f1]\n",
    "})\n",
    "\n",
    "# Calculate additional metrics for all models\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for pred in [xgb_pred, rf_pred, lr_pred, svm_pred, mlp_pred]:\n",
    "    precision_scores.append(precision_score(y_test, pred, average='weighted', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, pred, average='weighted', zero_division=0))\n",
    "\n",
    "results_df['Precision'] = precision_scores\n",
    "results_df['Recall'] = recall_scores\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON - LaBSE EMBEDDINGS\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Best model\n",
    "best_idx = results_df['F1 Score'].argmax()\n",
    "best_model = results_df.iloc[best_idx]['Model']\n",
    "print(f\"Best Model: {best_model} (F1: {results_df.iloc[best_idx]['F1 Score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Report for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from best model\n",
    "predictions_map = {\n",
    "    'XGBoost': xgb_pred,\n",
    "    'Random Forest': rf_pred,\n",
    "    'Logistic Regression': lr_pred,\n",
    "    'Linear SVM': svm_pred,\n",
    "    'MLP': mlp_pred\n",
    "}\n",
    "\n",
    "best_pred = predictions_map[best_model]\n",
    "\n",
    "print(f\"Classification Report - {best_model}\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, best_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 12))\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "labels = sorted(list(set(y_test.tolist() + best_pred.tolist())))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f'Confusion Matrix: LaBSE + {best_model}')\n",
    "plt.ylabel('True Tag')\n",
    "plt.xlabel('Predicted Tag')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('labse_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - 1.5*width, results_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "ax.bar(x - 0.5*width, results_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x + 0.5*width, results_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + 1.5*width, results_df['F1 Score'], width, label='F1 Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('LaBSE Embeddings: Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('labse_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Model comparison saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 scores for best model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report_dict = classification_report(y_test, best_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "# Extract per-class metrics\n",
    "class_metrics = []\n",
    "for class_name, metrics in report_dict.items():\n",
    "    if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        class_metrics.append({\n",
    "            'Class': class_name,\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1-Score': metrics['f1-score'],\n",
    "            'Support': metrics['support']\n",
    "        })\n",
    "\n",
    "class_df = pd.DataFrame(class_metrics).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nPer-Class Performance (Top 10):\")\n",
    "print(class_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nWorst Performing Classes (Bottom 5):\")\n",
    "print(class_df.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Get best model object\n",
    "models_map = {\n",
    "    'XGBoost': xgb_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Linear SVM': svm_model,\n",
    "    'MLP': mlp_model\n",
    "}\n",
    "\n",
    "best_model_obj = models_map[best_model]\n",
    "\n",
    "# Save\n",
    "model_dir = './labse_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving {best_model}...\")\n",
    "joblib.dump(best_model_obj, f'{model_dir}/best_classifier.pkl')\n",
    "joblib.dump(label_encoder, f'{model_dir}/label_encoder.pkl') if 'label_encoder' in dir() else None\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'best_model': best_model,\n",
    "    'embedding_model': EMBEDDING_MODEL,\n",
    "    'embedding_dim': train_embeddings.shape[1],\n",
    "    'num_classes': len(np.unique(y_train)),\n",
    "    'accuracy': results_df.iloc[best_idx]['Accuracy'],\n",
    "    'f1_score': results_df.iloc[best_idx]['F1 Score']\n",
    "}\n",
    "\n",
    "with open(f'{model_dir}/metadata.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {model_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Log to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logging to MLflow...\")\n",
    "mlflow_recorder = MLflowRecorder(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"embedding_dim\": train_embeddings.shape[1],\n",
    "    \"train_size\": len(train_df),\n",
    "    \"test_size\": len(test_df),\n",
    "    \"num_classes\": len(np.unique(y_train)),\n",
    "    \"best_model\": best_model\n",
    "}\n",
    "mlflow_recorder.log_params(params)\n",
    "\n",
    "# Metrics for all models\n",
    "metrics = {\n",
    "    \"xgb_accuracy\": xgb_acc,\n",
    "    \"xgb_f1\": xgb_f1,\n",
    "    \"rf_accuracy\": rf_acc,\n",
    "    \"rf_f1\": rf_f1,\n",
    "    \"lr_accuracy\": lr_acc,\n",
    "    \"lr_f1\": lr_f1,\n",
    "    \"svm_accuracy\": svm_acc,\n",
    "    \"svm_f1\": svm_f1,\n",
    "    \"mlp_accuracy\": mlp_acc,\n",
    "    \"mlp_f1\": mlp_f1,\n",
    "    \"best_accuracy\": results_df['Accuracy'].max(),\n",
    "    \"best_f1\": results_df['F1 Score'].max()\n",
    "}\n",
    "mlflow_recorder.log_metrics(metrics)\n",
    "\n",
    "# Artifacts\n",
    "mlflow_recorder.log_artifact('labse_confusion_matrix.png')\n",
    "mlflow_recorder.log_artifact('labse_model_comparison.png')\n",
    "\n",
    "# Tags\n",
    "mlflow_recorder.set_tag(\"model_type\", \"labse_classification\")\n",
    "mlflow_recorder.set_tag(\"best_classifier\", best_model)\n",
    "\n",
    "mlflow_recorder.end_run()\n",
    "print(\"MLflow logging complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY: LaBSE CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Embedding Dimension: {train_embeddings.shape[1]}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train: {len(train_df)} | Test: {len(test_df)}\")\n",
    "print(f\"  Classes: {len(np.unique(y_train))}\")\n",
    "print(f\"\\nBest Classifier: {best_model}\")\n",
    "print(f\"  Accuracy:  {results_df.iloc[best_idx]['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results_df.iloc[best_idx]['Precision']:.4f}\")\n",
    "print(f\"  Recall:    {results_df.iloc[best_idx]['Recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results_df.iloc[best_idx]['F1 Score']:.4f}\")\n",
    "print(f\"\\nModel saved to: {model_dir}/\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
